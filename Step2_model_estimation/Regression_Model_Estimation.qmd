---
title: "Regression Model Estimation"
format: 
  html:
    fig-dpi: 300
    fig-width: 7
    fig-height: 5
    code-fold: true
    toc: true
    toc-depth: 3
editor_options: 
  chunk_output_type: console
---



# 1 Input-preprocessing

## 1.1 load necessary packages

```{R include=FALSE}
# load package
library(readxl)
library(xlsx)
library(tidyverse)
library(factoextra)

# perform cluster analysis
library(quantreg)
library(Metrics)
library(cluster)

# perform Ridge regression in GLM and QRM, use the "glmnet" and "qrnn" packages
library(glmnet)
library(qrnn)

# perform Non-negative optiomal constraint
library(nnls)

```

## 1.2 load annual pop products and census data(WPP 2022 by UN)

```{R include=FALSE}
# *********** import **********************
year = 1990
year1990 <-
  read_excel("pop_3set_90to99.xlsx", sheet = paste0('year', year))

# year = 2001
# year1990 <- read_excel("pop_3set_00to20.xlsx",
# sheet = paste0('year',year))


# year1990 <- read_excel("2022_12/Country_cluster/pop_model_city_90to20.xlsx", sheet = paste0('year',year))

year1990 =  tidyDF_9099(year1990)
train_df = year1990

# *********** import median_composite  **********************
M5Cot = read.csv(
  file = "M5Cot_90_20_Un.csv",
  encoding = "UTF-8",
  header = 1,
  stringsAsFactors = FALSE,
  na.strings = c("...", "NA")
)

# *********** import regression_model_estimation  **********************
year = 2020
year1990 <-
  read_excel("pop_model_90to20.xlsx", sheet = paste0('year', year))
```

## 1.3 tidy data

```{R}

tidyDF_9099 = function(df) {
  print(paste0("Original nrow: ", nrow(df)))
  
  # rename col - 1990-1999
  names(df)[4] <- paste0("Grump")
  names(df)[5] <- paste0("GPW")
  names(df)[6] <- paste0("GHS")
  names(df)[7] <- paste0("UN")
  
  # exclude row less than zero
  df = df %>%
    filter((Grump > 0) & (GPW > 0) & (GHS > 0))
  print(paste0("Tidy nrow: ", nrow(df)))
  
  return(df)
  
}

tidyDF_00 = function(df) {
  print(paste0("Original nrow: ", nrow(df)))
  
  # rename col -  2000
  names(df)[4] <- paste0("GPW")
  names(df)[5] <- paste0("LS")
  names(df)[6] <- paste0("WP")
  names(df)[7] <- paste0("GHS")
  names(df)[8] <- paste0("Grump")
  names(df)[9] <- paste0("UN")
  # exclude row less than zero
  df = df %>%
    filter((Grump > 0) & (GPW > 0) & (GHS > 0) & (LS > 0) & (WP > 0))
  
  print(paste0("Tidy nrow: ", nrow(df)))
  
  return(df)
}

tidyDF_0120 = function(df) {
  print(paste0("Original nrow: ", nrow(df)))
  
  # rename col -  2001-2020
  names(df)[4] <- paste0("GPW")
  names(df)[5] <- paste0("LS")
  names(df)[6] <- paste0("WP")
  names(df)[7] <- paste0("UN")
  # exclude row less than zero
  df = df %>%
    filter((GPW > 0) & (LS > 0) & (WP > 0))
  
  print(paste0("Tidy nrow: ", nrow(df)))
  
  return(df)
}

year1990 =  tidyDF_9099(year1990)
train_df = year1990
#View(year1990)

```

# 2 Data processing

## 2.1 Cluster analysis

### 2.1.1 Calculate indicators

We selected four key indexes, namely the APE (Absolute Percentage Error), SE (Squared Error), SLE (Squared Logarithmic Error), and Dif (Difference) indexes. These indexes were chosen to facilitate a comprehensive comparison between different population data products and the corresponding census data in cluster analysis.

```{R }
getIndex_9099 = function(df){
  df = df %>%
    # APE = （Actual - Predict）/ Actual
    mutate(
      APE_Grump = abs(UN -Grump ) / UN,
      APE_GPW = abs(UN -GPW ) / UN,
      APE_GHS = abs(UN -GHS ) / UN
    ) %>%
    # SE = （Actual - Predict）^2
    mutate(
      SE_Grump = (UN -Grump )**2,
      SE_GPW = (UN -GPW )**2,
      SE_GHS = (UN -GHS )**2
  ) %>%
    # SLE = ( ln(1+x)-ln(1+y))^2
    mutate(
      SLE_Grump = ( log(1+UN) - log(1+Grump) ) ** 2,
      SLE_GPW = ( log(1+UN) - log(1+GPW) ) ** 2,
      SLE_GHS = ( log(1+UN) - log(1+GHS) ) ** 2
  ) %>%
    # 计算Dif = （ Predict - Actual）
    mutate(
      Dif_Grump = ( Grump - UN),
      Dif_GPW = ( GPW - UN),
      Dif_GHS = ( GHS - UN),
    )
  
  return(df)
}
getIndex_2000 = function(df){
  df = df %>%
    # 1 APE = （Actual - Predict）/ Actual
    mutate(
      APE_Grump = abs(UN -Grump ) / UN,
      APE_GPW = abs(UN -GPW ) / UN,
      APE_GHS = abs(UN -GHS ) / UN,
      APE_LS = abs(UN -LS ) / UN,
      APE_WP = abs(UN -WP ) / UN
    ) %>%
    # 2 SE = （Actual - Predict）^2
    mutate(
      SE_Grump = (UN -Grump )**2,
      SE_GPW = (UN -GPW )**2,
      SE_GHS = (UN -GHS )**2,
      SE_LS = (UN -LS )**2,
      SE_WP = (UN -WP )**2
  ) %>%
    # 3 SLE = ( ln(1+x)-ln(1+y))^2
    mutate(
      SLE_Grump = ( log(1+UN) - log(1+Grump) ) ** 2,
      SLE_GPW = ( log(1+UN) - log(1+GPW) ) ** 2,
      SLE_GHS = ( log(1+UN) - log(1+GHS) ) ** 2,
      SLE_LS = ( log(1+UN) - log(1+LS) ) ** 2,
      SLE_WP = ( log(1+UN) - log(1+WP) ) ** 2
  ) %>%
    # 4 Dif = （ Predict - Actual）
    mutate(
      Dif_Grump = ( Grump - UN),
      Dif_GPW = ( GPW - UN),
      Dif_GHS = ( GHS - UN),
      Dif_LS = ( LS - UN),
      Dif_WP = ( WP - UN)
    )
  
  return(df)
}
getIndex_0120 = function(df){
  df = df %>%
    # APE = （Actual - Predict）/ Actual
    mutate(
      APE_WP = abs(UN -WP ) / UN,
      APE_LS = abs(UN -LS ) / UN,
      APE_GPW = abs(UN -GPW ) / UN
    ) %>%
    # SE = （Actual - Predict）^2
    mutate(
      SE_WP = (UN -WP )**2,
      SE_LS = (UN -LS )**2,
      SE_GPW = (UN -GPW )**2
  ) %>%
    # SLE = ( ln(1+x)-ln(1+y))^2
    mutate(
      SLE_WP = ( log(1+UN) - log(1+WP) ) ** 2,
      SLE_LS = ( log(1+UN) - log(1+LS) ) ** 2,
      SLE_GPW = ( log(1+UN) - log(1+GPW) ) ** 2
  ) %>%
    # 计算Dif = （ Predict - Actual）
    mutate(
      Dif_WP = ( WP - UN),
      Dif_LS = ( LS - UN),
      Dif_GPW = ( GPW - UN),
    )
  
  return(df)
}
# get indicators
year1990 = getIndex_9099(year1990)
year1990 = getIndex_2000(year1990)
year1990 = getIndex_0120(year1990)

```

### 2.1.2 Normalization and K-means cluster analysis

**The clustering here is for each set of products to be clustered by country to determine the best country division.** 


Year ：1990-1999
z = year1990[,-c(1:7)]  all data
z = year1990[,c(8,11,14,17)]  grump
z = year1990[,c(9,12,15,18)]  gpw
z = year1990[,c(10,13,16,19)]  ghs

Year ：2000
z = year1990[,-c(1:9)]  all data
z = year1990[,c(10,15,20,25)]  grump
z = year1990[,c(11,16,21,26)]  gpw
z = year1990[,c(12,17,22,27)]  ghs
z = year1990[,c(13,18,23,28)]  ls
z = year1990[,c(14,19,24,29)]  wp

Year ：2001-2020 
z = year1990[,-c(1:7)]  all data
z = year1990[,c(8,11,14,17)]  wp
z = year1990[,c(9,12,15,18)]  ls
z = year1990[,c(10,13,16,19)]  gpw

```{R }

# Get the optimal cluster numbers
# input：data frame with indicators
# output：optimal cluster numbers of each pop products

getNClust = function(df){
  
  # normalization
  means = apply(df,2, mean) # 2-col data
  sds = apply(df, 2, sd) 
  nor = scale(df,center = means, scale = sds)
  
  # Get the optimal cluster numbers
  k = fviz_nbclust(nor, kmeans, method = "silhouette",print.summary=TRUE)
  t = k$data
  n = which.max(t$y)
  
  return(n)
}

# input：data frame with indicators,product name,year
# output：cluster analysis results
ClusterAnalysis_9099 = function(df,ProductName,year){

  if(ProductName  == "Grump"){
    
    # choose Grump
    z = df[,c(8,11,14,17)] 
    # normalization
    means = apply(z,2,mean) 
    sds = apply(z, 2, sd) 
    nor = scale(z,center = means, scale = sds)
  
    # get grump optimal cluster numbers
    n_center = getNClust(z)
    
    # K-Means Cluster Analysis
    set.seed(123)
    k2 <- kmeans(nor, centers = n_center, nstart = 25)
    
    # save as png format in the Fig directory
    png_name = paste0("D://Research//Process_Fig//PopCluster//",ProductName,"_cluster_",year,".png")
    
    # a cluster figure we want to save
    fviz_cluster(k2, data = nor)
    
    # save ggplot
    ggsave(png_name , width = 7, height = 5) 
    
    
  }
  else if(ProductName  == "Gpw"){
    # choose Gpw
    z = df[,c(9,12,15,18)]

    means = apply(z,2,mean) 
    sds = apply(z, 2, sd) 
    nor = scale(z,center = means, scale = sds)
    
    #  get gpw optimal cluster numbers
    n_center = getNClust(z)
    
    # K-Means Cluster Analysis
    set.seed(123)
    k2 <- kmeans(nor, centers = n_center, nstart = 25)
    
    # save as png format in the Fig directory
    png_name = paste0("D://Research//Process_Fig//PopCluster//",ProductName,"_cluster_",year,".png")
    
    # a cluster figure we want to save
    fviz_cluster(k2, data = nor)
    
    # save ggplot
    ggsave(png_name , width = 7, height = 5)
  } 
  else if (ProductName  == "Ghs"){
    # choose ghs
    z = df[,c(10,13,16,19)]

        means = apply(z,2,mean) 
    sds = apply(z, 2, sd) 
    nor = scale(z,center = means, scale = sds)
    
    #  get ghs optimal cluster numbers
    n_center = getNClust(z)
    
    # K-Means Cluster Analysis
    set.seed(123)
    k2 <- kmeans(nor, centers = n_center, nstart = 25)
    
    # save as png format in the Fig directory
    png_name = paste0("D://Research//Process_Fig//PopCluster//",ProductName,"_cluster_",year,".png")
    
    # a cluster figure we want to save
    fviz_cluster(k2, data = nor)
    
    # save ggplot
    ggsave(png_name , width = 7, height = 5)
  }
  
  
}
ClusterAnalysis_2000 = function(df,ProductName,year){

  if(ProductName  == "Grump"){
    
    # choose Grump
    z = df[,c(10,15,20,25)] 

    means = apply(z,2,mean) 
    sds = apply(z, 2, sd) 
    nor = scale(z,center = means, scale = sds)
  
    n_center = getNClust(z)
    
    # K-Means Cluster Analysis
    set.seed(123)
    k2 <- kmeans(nor, centers = n_center, nstart = 25)
    
    # save as png format in the Fig directory
    png_name = paste0("D://Research//Process_Fig//PopCluster//",ProductName,"_cluster_",year,".png")
    
    # a cluster figure we want to save
    fviz_cluster(k2, data = nor)
    
    # save ggplot
    ggsave(png_name , width = 7, height = 5) 
    
    
  }
  else if(ProductName  == "Gpw"){
    # choose Gpw
    z = df[,c(11,16,21,26)]

    means = apply(z,2,mean)
    sds = apply(z, 2, sd) 
    nor = scale(z,center = means, scale = sds)
    
    n_center = getNClust(z)
    
    # K-Means Cluster Analysis
    set.seed(123)
    k2 <- kmeans(nor, centers = n_center, nstart = 25)
    
    # save as png format in the Fig directory
    png_name = paste0("D://Research//Process_Fig//PopCluster//",ProductName,"_cluster_",year,".png")
    
    # a cluster figure we want to save
    fviz_cluster(k2, data = nor)
    
    # save ggplot
    ggsave(png_name , width = 7, height = 5)
  } 
  else if (ProductName  == "Ghs"){

    z = df[,c(12,17,22,27)]

    means = apply(z,2,mean) 
    sds = apply(z, 2, sd) 
    nor = scale(z,center = means, scale = sds)
    
    n_center = getNClust(z)
    
    # K-Means Cluster Analysis
    set.seed(123)
    k2 <- kmeans(nor, centers = n_center, nstart = 25)
    
    # save as png format in the Fig directory
    png_name = paste0("D://Research//Process_Fig//PopCluster//",ProductName,"_cluster_",year,".png")
    
    # a cluster figure we want to save
    fviz_cluster(k2, data = nor)
    
    # save ggplot
    ggsave(png_name , width = 7, height = 5)
  }
  else if (ProductName  == "ls"){

    z = df[,c(13,18,23,28)]

    means = apply(z,2,mean) 
    sds = apply(z, 2, sd) 
    nor = scale(z,center = means, scale = sds)

    n_center = getNClust(z)
    
    # K-Means Cluster Analysis
    set.seed(123)
    k2 <- kmeans(nor, centers = n_center, nstart = 25)
    
    # save as png format in the Fig directory
    png_name = paste0("D://Research//Process_Fig//PopCluster//",ProductName,"_cluster_",year,".png")
    
    # a cluster figure we want to save
    fviz_cluster(k2, data = nor)
    
    # save ggplot
    ggsave(png_name , width = 7, height = 5)
  }
  else if (ProductName  == "wp"){

    z = df[,c(14,19,24,29)]

    means = apply(z,2,mean) 
    sds = apply(z, 2, sd) 
    nor = scale(z,center = means, scale = sds)
    
    n_center = getNClust(z)
    
    # K-Means Cluster Analysis
    set.seed(123)
    k2 <- kmeans(nor, centers = n_center, nstart = 25)
    
    # save as png format in the Fig directory
    png_name = paste0("D://Research//Process_Fig//PopCluster//",ProductName,"_cluster_",year,".png")
    
    # a cluster figure we want to save
    fviz_cluster(k2, data = nor)
    
    # save ggplot
    ggsave(png_name , width = 7, height = 5)
  }
  
  
}
ClusterAnalysis_0120 = function(df,ProductName,year){

  if(ProductName  == "wp"){
    
    z = df[,c(8,11,14,17)] 

    means = apply(z,2,mean)
    sds = apply(z, 2, sd) 
    nor = scale(z,center = means, scale = sds)
  
    n_center = getNClust(z)
    
    # K-Means Cluster Analysis
    set.seed(123)
    k2 <- kmeans(nor, centers = n_center, nstart = 25)
    
    # save as png format in the Fig directory
    png_name = paste0("D://Research//Process_Fig//PopCluster//",ProductName,"_cluster_",year,".png")
    
    # a cluster figure we want to save
    fviz_cluster(k2, data = nor)
    
    # save ggplot
    ggsave(png_name , width = 7, height = 5) 
    
    
  }
  else if(ProductName  == "ls"){

    z = df[,c(9,12,15,18)]

    means = apply(z,2,mean) 
    sds = apply(z, 2, sd) 
    nor = scale(z,center = means, scale = sds)
    
    n_center = getNClust(z)
    
    # K-Means Cluster Analysis
    set.seed(123)
    k2 <- kmeans(nor, centers = n_center, nstart = 25)
    
    # save as png format in the Fig directory
    png_name = paste0("D://Research//Process_Fig//PopCluster//",ProductName,"_cluster_",year,".png")
    
    # a cluster figure we want to save
    fviz_cluster(k2, data = nor)
    
    # save ggplot
    ggsave(png_name , width = 7, height = 5)
  } 
  else if (ProductName  == "Gpw"){

    z = df[,c(10,13,16,19)]

    means = apply(z,2,mean)
    sds = apply(z, 2, sd) 
    nor = scale(z,center = means, scale = sds)
    
    n_center = getNClust(z)
    
    # K-Means Cluster Analysis
    set.seed(123)
    k2 <- kmeans(nor, centers = n_center, nstart = 25)
    
    # save as png format in the Fig directory
    png_name = paste0("D://Research//Process_Fig//PopCluster//",ProductName,"_cluster_",year,".png")
    
    # a cluster figure we want to save
    fviz_cluster(k2, data = nor)
    
    # save ggplot
    ggsave(png_name , width = 7, height = 5)
  }
  
  
}
```


check the results:

```{R }

# 1990-1999
P_name = c("Grump", "Gpw", "Ghs")
for(i in 1:3){
  ClusterAnalysis_9099(year1990,P_name[i],year)
}

# 2000
P_name = c("Grump", "Gpw", "Ghs","ls","wp")
for(i in 1:5){
  ClusterAnalysis_2000(year1990,P_name[i],year)
}

# 2001-2020
P_name = c("wp", "ls", "Gpw")
for(i in 1:3){
  ClusterAnalysis_0120(year1990,P_name[i],year)
}
```

According to the figure above, the number corresponds to the country represented by the row number. The negative direction means that the product is seriously overvalued in this country; the positive direction means that the product is seriously undervalued in this country.

### 2.1.3 Perform cluster analysis: 2001-2020

```{R}
for (i in 2002:2020){
  
  # import orginal data
  year = i
  year1990 <- read_excel("pop_3set_00to20.xlsx", sheet = paste0('year',year))
  
  year1990 =  tidyDF(year1990)
  
  year1990 = getIndex_0120(year1990)
  
  # 2001-2020
  P_name = c("wp", "ls", "Gpw")
  for(i in 1:3){
    ClusterAnalysis_0120(year1990,P_name[i],year)}
  
}
```

## 2.2 Regression Model

### 2.2.1 Determine the date for training and test

The training data comes from the clustering results, and the country class with better clustering results in the clustering is selected as the training data, and the rest are used as the test data.


```{R }

# train data：clustered-better
getData_9099 = function(df,year){
  if(year == 1990){
    clust_list=c(88,89,98,118,183,123,150,164,202)
    
  }else if(year == 1991){
    clust_list=c(37,89,118,164,202,88,98,150,183,123)
 
  }else if(year == 1992){
    clust_list=c(37,89,164,202,150)

  }else if(year == 1993){
    clust_list=c(118,164,202,37,150,183,123)
      
  }else if(year == 1994){
    clust_list=c(89,164,202,37,150,98,88,183,123,118)
    
  }else if(year == 1995){
    clust_list=c(89,165,203, 88,37,150)
    
  }else if(year == 1996){
    clust_list=c(37,89,165,203,150,88)
    
  }else if(year == 1997){
    clust_list=c(89 ,165,203,150,37,88)
    
  }else if(year == 1998){
    clust_list=c(89,165,203,184,123,118)
    
  }else if(year == 1999){
    clust_list=c(89,165,203,150,184,123,118)
    
  }
  
  train.data <- df[-clust_list,c(4:7)]
  test.data <- df[clust_list,c(4:7)]
  
  # Parameters are returned as a list
  return(list(x = train.data, y = test.data)) 
}

getData_00 = function(df,year){
  
  if(year == 2000){
    clust_list=c(89,123,184,118,150,84)
  }
  
  train.data <- df[-clust_list,c(4:9)]
  test.data <- df[clust_list,c(4:9)]
  
  # Parameters are returned as a list
  return(list(x = train.data, y = test.data))
}

getData_0120 = function(df,year){
  
  if(year >= 2001 && year <= 2003){
    clust_list=c(89,123,184,118)
    
  }else if(year == 2004){
    clust_list=c(89)

  }else if(year == 2005 || year == 2006){
    clust_list=c(89,118,123,184,198)

  }else if(year == 2007){
    clust_list=c(89,123,184,118)
    
  }else if(year >= 2008 && year <= 2010){
    clust_list=c(89,118,123,184,198)
    
  }else if(year >= 2011 && year <= 2015){
    clust_list=c(89,118,123,184)
    
  }else if(year == 2016 || year == 2017){
    clust_list=c(37,40,89,118,123,184)
    
  }else if(year == 2018){
    clust_list=c(37,40,118,123)
    
  }else if(year == 2019 || year == 2020){
    clust_list=c(389,118,123)
    
  }
  
  train.data <- df[-clust_list,c(4:7)]
  test.data <- df[clust_list,c(4:7)]
  # Parameters are returned as a list
  
  return(list(x = train.data, y = test.data))
}

# Call the function to split the training data and test data
Data = getData_9099(train_df,year)

Data = getData_00(train_df,year)

Data = getData_0120(train_df,year)

train.data = Data$x
test.data <- Data$y

```


### 2.2.2 Model selection and parameter output


```{R}
# ****************** Model selection and parameter output ******************
# Model_name = c("glm","rq")
# year = 1990:2020
# df - The data frame used for training, note that different countries use different products in different years
# df2 - The data frame used for test, note that different countries use different products in different years
# Output: composite coefficients, test data predicted population in tabular form

Model_train = function(Model_name,year,pop,df,df2){

  if(Model_name == 'glm' && year < 2000){# GLM，1990-1999
    
    Fit = switch(pop,
      glm(UN ~  GHS + Grump,data = df),
      glm(UN ~  GHS + GPW,data = df),
      glm(UN ~  GPW + Grump,data = df),
      glm(UN ~  GHS + Grump + GPW,data = df)
      )
    cof =  Fit$coefficients
    test = switch(pop,
      (df2$GHS)*cof[2]+(df2$Grump)*cof[3],
      (df2$GHS)*cof[2]+(df2$GPW)*cof[3],
      (df2$GPW)*cof[2]+(df2$Grump)*cof[3],
      (df2$GHS)*cof[2]+(df2$Grump)*cof[3]+(df2$GPW)*cof[4])
  
  }else if(Model_name == 'glm' && year == 2000){# GLM，2000
    Fit = switch(pop,
      glm(UN ~  LS + WP,data = df),
      glm(UN ~  LS + GPW,data = df),
      glm(UN ~  GPW + WP,data = df),
      glm(UN ~  LS + WP + GPW + GHS + Grump,data = df))
    cof =  Fit$coefficients
    test = switch(pop,
      (df2$LS)*cof[2]+(df2$WP)*cof[3],
      (df2$LS)*cof[2]+(df2$GPW)*cof[3],
      (df2$GPW)*cof[2]+(df2$WP)*cof[3],
      (df2$LS)*cof[2]+ (df2$WP)*cof[3]+(df2$GPW)*cof[4]+(df2$GHS)*cof[5]+(df2$Grump)*cof[6] )
    
  }else if(Model_name == 'glm' && year > 2000){# GLM，2001-2020
    Fit = switch(pop,
      glm(UN ~  LS + WP,data = df),
      glm(UN ~  LS + GPW,data = df),
      glm(UN ~  GPW + WP,data = df),
      glm(UN ~  LS + WP + GPW,data = df))
    cof =  Fit$coefficients
    test = switch(pop,
      (df2$LS)*cof[2]+(df2$WP)*cof[3],
      (df2$LS)*cof[2]+(df2$GPW)*cof[3],
      (df2$GPW)*cof[2]+(df2$WP)*cof[3],
      (df2$LS)*cof[2]+(df2$WP)*cof[3]+(df2$GPW)*cof[4])
    
  }else if(Model_name == 'rq' && year < 2000){# QRM，1990-1999
    Fit = switch(pop,
      rq(UN ~  GHS + Grump,data = df),
      rq(UN ~  GHS + GPW,data = df),
      rq(UN ~  GPW + Grump,data = df),
      rq(UN ~  GHS + Grump + GPW,data = df))
    cof =  Fit$coefficients
    test = switch(pop,
      (df2$GHS)*cof[2]+(df2$Grump)*cof[3],
      (df2$GHS)*cof[2]+(df2$GPW)*cof[3],
      (df2$GPW)*cof[2]+(df2$Grump)*cof[3],
      (df2$GHS)*cof[2]+(df2$Grump)*cof[3]+(df2$GPW)*cof[4])
    
    }else if(Model_name == 'rq' && year == 2000){# QRM，2000
    Fit = switch(pop,
      rq(UN ~  LS + WP,data = df),
      rq(UN ~  LS + GPW,data = df),
      rq(UN ~  GPW + WP,data = df),
      rq(UN ~  LS + WP + GPW + GHS + Grump,data = df))
    cof =  Fit$coefficients
    test = switch(pop,
      (df2$LS)*cof[2]+(df2$WP)*cof[3],
      (df2$LS)*cof[2]+(df2$GPW)*cof[3],
      (df2$GPW)*cof[2]+(df2$WP)*cof[3],
      (df2$LS)*cof[2]+ (df2$WP)*cof[3]+(df2$GPW)*cof[4]+(df2$GHS)*cof[5]+(df2$Grump)*cof[6] )
    
  }else if(Model_name == 'rq' && year > 2000){# QRM，2001-2020
    Fit = switch(pop,
      rq(UN ~  LS + WP,data = df),
      rq(UN ~  LS + GPW,data = df),
      rq(UN ~  GPW + WP,data = df),
      rq(UN ~  LS + WP + GPW,data = df))
     cof =  Fit$coefficients
    test = switch(pop,
      (df2$LS)*cof[2]+(df2$WP)*cof[3],
      (df2$LS)*cof[2]+(df2$GPW)*cof[3],
      (df2$GPW)*cof[2]+(df2$WP)*cof[3],
      (df2$LS)*cof[2]+(df2$WP)*cof[3]+(df2$GPW)*cof[4])
    
    }

  x = round(cof, digits = 3)
  y = round(test, digits = 5)

  return(list(x = x, y = y)) # Parameters are returned as a list
}

Model_name = c("glm","rq")

#* The following lines are test codes, not official codes *#
# model_cof <- Model_train(Model_name[2],year,pop=4,train.data,test.data)
# cof = model_cof$x
# cof

# Test：Fit a Ridge regression model
# y_train = as.matrix(train.data$UN)
# x_train = as.matrix(cbind(train.data$Grump,train.data$GPW,train.data$GHS))
# y_test = as.matrix(test.data$UN)
# x_test = as.matrix(cbind(test.data$Grump,test.data$GPW,test.data$GHS))
# fit <- glmnet(x_train, y_train, alpha = 0, lambda = 0.1)
# y_pred <- predict(fit, newx = x_test)
# mse <- mean((y_test - y_pred)^2)


```
Optimize QR Model with Non-Negativity Constraints

```{R}
# Define the quantile regression function to be optimized
quantile_loss <- function(coefs, data, quantile) {
  y <- data$UN
  # X <- data[ , -c("UN")]
  # Select all columns except "UN"
  X <- data %>% select(-UN)
  
  X <- as.matrix(X)
  coefs <- as.numeric(coefs)
  
  # Calculate the quantile loss function with non-negativity constraint
  quantile_loss <- sum(pmax(y - X %*% coefs, 0)) * quantile +
                  sum(pmax(X %*% coefs - y, 0)) * (1 - quantile)
  return(quantile_loss)
}



# Set up for non-negativity constraints
variables <- colnames(train.data %>% select(-UN) )


# Create an empty data frame to store the results
results_df_0120 <- data.frame(
  Initial_Quantile = numeric(0),
  GPW_Coefficient = numeric(0),
  LS_Coefficient = numeric(0),
  WP_Coefficient = numeric(0),
  Model_Convergence = numeric(0)
)

results_df_00 <- data.frame(
  Initial_Quantile = numeric(0),
  GPW_Coefficient = numeric(0),
  LS_Coefficient = numeric(0),
  WP_Coefficient = numeric(0),
  GHS_Coefficient = numeric(0),
  Grump_Coefficient = numeric(0),
  Model_Convergence = numeric(0)
)

results_df_9099 <- data.frame(
  Initial_Quantile = numeric(0),
  Grump_Coefficient = numeric(0),
  GPW_Coefficient = numeric(0),
  GHS_Coefficient = numeric(0),
  Model_Convergence = numeric(0)
)

optimize_quantile_Manu_0120 = function(initial_quantile,train.data,
                                  quantile_loss,variables,
                                  results_df){
  
  initial_coefs <- c(0.1, 0.1, 0.1)  # For 2001-2020
  
  # Optimize the QR model with non-negativity constraints
  result <- optim(par = initial_coefs, 
                fn = quantile_loss, 
                data = train.data, 
                quantile = initial_quantile, 
                # Specifies the optimization method or algorithm to use.
                # L-BFGS-B is often a good default choice for constrained optimization problems, 
                # especially when you have bounds on the parameter values. It's efficient and reliable.
                method = "L-BFGS-B", 
                # To enforce non-negativity, set lower bounds to 0
                # lower = 0, 
                lower = c(0, 0, 0),
                # A list of control parameters for the optimization algorithm. 
                # For example: set the maximum number of iterations with maxit
                control = list(maxit = 10000) )
  
  # coefs_with_variables <- data.frame(Variable = variables, 
  #                                  Initial_Coefs = initial_coefs, 
  #                                  Coefficient = sprintf("%.3f", result$par))
  coefs_with_variables <- data.frame(
    Initial_Quantile = initial_quantile,
    GPW_Coefficient = round(result$par,3)[1],
    LS_Coefficient = round(result$par,3)[2],
    WP_Coefficient = round(result$par,3)[3],
    Model_Convergence = result$convergence
  )
  
  results_df <- rbind(results_df, coefs_with_variables)
  
  print("----------------------------------------------")
  print(paste0("The initial quantile: ",initial_quantile))
  print(coefs_with_variables)
  print(paste0("The model convergence: ",result$convergence,
             " (`0` means that the optimization process converged successfully)."))
  
  return(results_df)
}


optimize_quantile_Manu_00 = function(initial_quantile,train.data,
                                  quantile_loss,variables,
                                  results_df_00){
  
  initial_coefs <- c(0.1, 0.1, 0.1, 0.1, 0.1)  # For 2000
  
  # Optimize the QR model with non-negativity constraints
  result <- optim(par = initial_coefs, 
                fn = quantile_loss, 
                data = train.data, 
                quantile = initial_quantile, 
                method = "L-BFGS-B", 
                lower = c(0, 0, 0, 0, 0),
                control = list(maxit = 10000) )
  
  coefs_with_variables <- data.frame(
    Initial_Quantile = initial_quantile,
    GPW_Coefficient = round(result$par,3)[1],
    LS_Coefficient = round(result$par,3)[2],
    WP_Coefficient = round(result$par,3)[3],
    GHS_Coefficient = round(result$par,3)[4],
    Grump_Coefficient = round(result$par,3)[5],
    Model_Convergence = result$convergence
  )
  
  results_df_00 <- rbind(results_df_00, coefs_with_variables)
  
  print("-----------------------------------------------")
  print(paste0("The initial quantile: ",initial_quantile))
  print(coefs_with_variables)
  print(paste0("The model convergence: ",result$convergence,
             " (`0` means that the optimization process converged successfully)."))
  
  return(results_df_00)
}


optimize_quantile_Manu_9099 = function(initial_quantile,train.data,
                                  quantile_loss,variables,
                                  results_df){
  
  initial_coefs <- c(0.1, 0.1, 0.1)  # For 1990-1999
  
  # Optimize the QR model with non-negativity constraints
  result <- optim(par = initial_coefs, 
                fn = quantile_loss, 
                data = train.data, 
                quantile = initial_quantile, 
                method = "L-BFGS-B", 
                lower = c(0, 0, 0),
                control = list(maxit = 10000) )

  coefs_with_variables <- data.frame(
    Initial_Quantile = initial_quantile,
    Grump_Coefficient = round(result$par,3)[1],
    GPW_Coefficient = round(result$par,3)[2],
    GHS_Coefficient = round(result$par,3)[3],
    Model_Convergence = result$convergence
  )
  
  results_df <- rbind(results_df, coefs_with_variables)
  
  print("-----------------------------------------------")
  print(paste0("The initial quantile: ",initial_quantile))
  print(coefs_with_variables)
  print(paste0("The model convergence: ",result$convergence,
             " (`0` means that the optimization process converged successfully)."))
  
  return(results_df)
}


quantiles <-  seq(0.01, 1, by = 0.01)

for(i in quantiles){
  
  # 2001-2020
  results_df_0120  = optimize_quantile_Manu_0120(i,train.data,
                                       quantile_loss,variables,
                                       results_df_0120)

  # 2000
  # results_df_00  = optimize_quantile_Manu_00(i,train.data,
  #                                      quantile_loss,variables,
  #                                      results_df_00)

  # 1990-1999
  # results_df_9099  = optimize_quantile_Manu_9099(i,train.data,
  #                                      quantile_loss,variables,
  #                                      results_df_9099)
  
}
# save the results_df
result_table_path = ""
write.xlsx(results_df_0120, result_table_path, 
           sheetName = paste0("Coefficient_",year), append = TRUE,
           row.names = FALSE)

```



### 2.2.3 Accuracy test

```{R}
# Single year's Accuracy test
# Function: Accuracy_Rsq_RMSE_MAE_RE()
# Input: composited data, census data, year
# Output：list(Rsq，RMSE，MAE，RE)
Accuracy_Rsq_RMSE_MAE_RE = function(full,census,year){

  # parameter storage list
  list <- c()

  # R^2
  fit <- lm(full ~ census)
  summary <- summary(fit)
  list = append( list, summary$r.squared)

  # RMSE、MAE
  rmse = rmse(full, census)
  mae = mae(full, census)
  list = append( list,rmse)
  list = append( list,mae)

  # RE(Relative Entropy)
  p_full = ecdf(full) # Convert to probability distribution function
  p_census = ecdf(census)
  relativeEntrop = function(probDensity1, probDensity2) {
    return(sum(
      log(probDensity1 ^ probDensity1) - log(probDensity2 ^ probDensity1)
    ))}
  RelaEntropy_full <- relativeEntrop(p_census(census), p_full(full))
  list = append( list, RelaEntropy_full)

  # return parameter list
  list_tidy = round(list, digits = 4)
  names(list_tidy) <- c("R^2","RMSE","MAE","RE")
  return(list_tidy)
}

# Test data composite results
full = model_cof$y

# Test data itself - grump
full = test.data$Grump

# Test data itself - GPW
full = test.data$GPW

# Test data itself - GHS
full = test.data$GHS
      
# The census data corresponding to the test data
census = test.data$UN

# call precision function
Accuracy_list = Accuracy_Rsq_RMSE_MAE_RE(full,census,year)
Accuracy_list

```


### 2.2.4 Repeat training

```{R}
# ******************* cross-validation, repeated training *******************

# Input: training data frame, training times, model name, training year, population products dataframe
# Output: Print the four precision indicators and the population product coefficient of the corresponding year

Train_repeat = function(df,train_times,name,year,pop){

  # Internal parameter definition
  Model_name = name # c("glm","rq")
  popIndex = pop # Numbers: 1/2/3/4 represent different population product groups
  trainDF = df
  year = year

  # Parameters required for output: 
  #           the first 4 are precision indicators,
  #           and the last 5 are required result coefficients.
  MAE_mean <- c() # Mean absolute error
  Rsq_mean <- c() 
  RMSE_mean <- c()
  RE_mean <- c()  # Relative entropy
  cof_GHS_mean <- c()
  cof_GPW_mean <- c()
  cof_Grump_mean <- c()
  cof_LS_mean <- c()
  cof_WP_mean <- c()

  # *************** Repeat training + testing **********************
  for(k in 1:train_times){
    
    # Output list
    MAE_list <- c() 
    Rsq_list <- c()
    RMSE_list <- c()
    RE_list <- c()  
    cof_GHS_list <- c()
    cof_GPW_list <- c()
    cof_Grump_list <- c()
    cof_LS_list <- c()
    cof_WP_list <- c()

    # Randomly shuffle the data
    trainDF <- trainDF[sample(nrow(trainDF)), ]

    # Create 10-fold data
    folds <- cut(seq(1, nrow(trainDF)),breaks = 10, labels = FALSE)

    # Perform cross-validation
    for (i in 1:10) {

      # split data
      testIndexes <- which(folds == i, arr.ind = TRUE)
      testData <- trainDF[testIndexes,]
      trainData <- trainDF[-testIndexes,]


      # Training: including GLM model and QR
      
      # Call the function Model(Model_name,year,pop,df,df2)
      # Function returns: x-coefficient, y-test accuracy four indicators
      model_cof <-   Model_train(Model_name,year,popIndex,trainData,testData)
      
      # Output training parameters: here need to be changed according to the year
      if (year <2000 ){
        cof_GHS_list <- append(cof_GHS_list, model_cof$x[2])
        cof_Grump_list <- append(cof_Grump_list, model_cof$x[3])
        cof_GPW_list <- append(cof_GPW_list, model_cof$x[4]) 
        
      }else if(year == 2000){
        cof_LS_list <- append(cof_LS_list, model_cof$x[2])
        cof_WP_list <- append(cof_WP_list, model_cof$x[3])
        cof_GPW_list <- append(cof_GPW_list, model_cof$x[4]) 
        cof_GHS_list <- append(cof_GHS_list, model_cof$x[5])
        cof_Grump_list <- append(cof_Grump_list, model_cof$x[6])

      }else if(year > 2000){
        cof_LS_list <- append(cof_LS_list, model_cof$x[2])
        cof_WP_list <- append(cof_WP_list, model_cof$x[3])
        cof_GPW_list <- append(cof_GPW_list, model_cof$x[4]) 
      }
   
      
       # Test: compare the test data with the census data
       # Test data composite results
       full = model_cof$y
       
       # The census data corresponding to the test data
       census = testData$UN
       
       # call precision function
      Accuracy_list = Accuracy_Rsq_RMSE_MAE_RE(full,census,year)
      Rsq_list <- append(Rsq_list, Accuracy_list[1]) 
      RMSE_list <- append(RMSE_list, Accuracy_list[2]) 
      MAE_list <- append(MAE_list, Accuracy_list[3]) 
      RE_list <- append(MAE_list, Accuracy_list[4]) 
    }
    
    # get the mean of the parameters
    Rsq_mean <- append(Rsq_mean, mean(Rsq_list))
    RMSE_mean <- append(RMSE_mean, mean(RMSE_list))
    MAE_mean <- append(MAE_mean, mean(MAE_list))
    RE_mean <- append(RE_mean, mean(RE_list)) 
    
    # Adjust the output parameters according to the year
    if ( year < 2000){
      cof_GHS_mean <- append(cof_GHS_mean, mean(cof_GHS_list))
      cof_GPW_mean <- append(cof_GPW_mean, mean(cof_GPW_list))
      cof_Grump_mean <- append(cof_Grump_mean, mean(cof_Grump_list))
      
    }else if (year == 2000){
      cof_WP_mean <- append(cof_WP_mean, mean(cof_WP_list))
      cof_LS_mean <- append(cof_LS_mean, mean(cof_LS_list))
      cof_GHS_mean <- append(cof_GHS_mean, mean(cof_GHS_list))
      cof_GPW_mean <- append(cof_GPW_mean, mean(cof_GPW_list))
      cof_Grump_mean <- append(cof_Grump_mean, mean(cof_Grump_list))
      
    }else if (year > 2000){
      cof_WP_mean <- append(cof_WP_mean, mean(cof_WP_list))
      cof_LS_mean <- append(cof_LS_mean, mean(cof_LS_list))
      cof_GPW_mean <- append(cof_GPW_mean, mean(cof_GPW_list))
    }


  }

  print(paste0("Rsq-mean(k times 10 folds validation): ",
               round(mean(Rsq_mean),digits = 5)))
  print(paste0("RMSE-mean(k times 10 folds validation): ",
               round(mean(RMSE_mean),digits = 2)))
  print(paste0("MAE-mean(k times 10 folds validation): ",
               round(mean(MAE_mean),digits = 2)))
  print(paste0("RE-mean(k times 10 folds validation): ",
               round(mean(RE_mean),digits = 2)))

  if ( year < 2000){
    print(paste0("cof-GHS mean(k times 10 folds validation): ",
               round(mean(cof_GHS_mean),digits = 3)))
    print(paste0("cof-GPW mean(k times 10 folds validation): ",
               round(mean(cof_GPW_mean),digits = 3)))
    print(paste0("cof-Grump mean(k times 10 folds validation): ",
               round(mean(cof_Grump_mean),digits = 3)))
    
    }else if (year == 2000){
      print(paste0("cof-LS mean(k times 10 folds validation): ",
               round(mean(cof_LS_mean),digits = 3)))
      print(paste0("cof-WP mean(k times 10 folds validation): ",
               round(mean(cof_WP_mean),digits = 3)))
      print(paste0("cof-GHS mean(k times 10 folds validation): ",
               round(mean(cof_GHS_mean),digits = 3)))
      print(paste0("cof-GPW mean(k times 10 folds validation): ",
               round(mean(cof_GPW_mean),digits = 3)))
      print(paste0("cof-Grump mean(k times 10 folds validation): ",
               round(mean(cof_Grump_mean),digits = 3)))
      
    }else if (year > 2000){
      print(paste0("cof-LS mean(k times 10 folds validation): ",
               round(mean(cof_LS_mean),digits = 3)))
      print(paste0("cof-WP mean(k times 10 folds validation): ",
               round(mean(cof_WP_mean),digits = 3)))
      print(paste0("cof-GPW mean(k times 10 folds validation): ",
               round(mean(cof_GPW_mean),digits = 3)))
    }
  
}


```


#### 2.2.4.1 multiple years of training
```{R}

# 1990:1999
for (annum in 1990:1999) {

  year1990 <- read_excel("pop_3set_90to99.xlsx", sheet = paste0('year',annum))
  

  year1990 =  tidyDF_9099(year1990)
  train_df = year1990
  
   # Training data: clustered
   # Call the function to split the training data and test data
   Data = getData_9099(train_df,annum)
   train.data = Data$x
   test.data <- Data$y
  
   # Train the model and save the parameters
  StoreOutput_coef(t = 200,annum)
}

# 2000
getOutput_2000 = function (annum){
  

  year1990 <- read_excel("pop_3set_00to20.xlsx", sheet = paste0('year',2000))
  
  year1990 =  tidyDF_00(year1990)
  train_df = year1990
  
  # Training data: clustered
  # Call the function to split the training data and test data
  Data = getData_00(train_df,annum)
  train.data = Data$x
  test.data <- Data$y
  
  StoreOutput_coef(t = 200,annum)
} 

getOutput_2000(annum = 2000)

# 2001:2020
for (annum in 2001:2020) {

  year1990 <- read_excel("pop_3set_00to20.xlsx", sheet = paste0('year',annum))
  
  year1990 =  tidyDF_0120(year1990)
  train_df = year1990
  
  # Training data: clustered
  # Call the function to split the training data and test data
  Data = getData_0120(train_df,annum)
  train.data = Data$x
  test.data <- Data$y
  
  StoreOutput_coef(t = 200,annum)
}
```

### 2.2.5 Repeated training accuracy comparison and plotting


```{R}
get_t_times_accuracy = function(t) {
  
  # 1990:1999
  for (annum in 1990:1999) {
    year1990 <-
      read_excel("pop_3set_90to99.xlsx", sheet = paste0('year', annum))
    
    
    year1990 =  tidyDF_9099(year1990)
    train_df = year1990
    
    
    Data = getData_9099(train_df, annum)
    train.data = Data$x
    test.data <- Data$y
    
    
    StoreOutput_coef(t, annum)
  }
  
  # 2000
  getOutput_2000 = function (annum) {
    filepath = paste0("pop_3set_00to20.xlsx")
    year1990 <- read_excel(filepath, sheet = paste0('year', 2000))
    
    
    year1990 =  tidyDF_00(year1990)
    train_df = year1990
    
    
    train.data_whole  <- train_df[, c(4:9)]
    
    
    Data = getData_00(train_df, annum)
    train.data = Data$x
    test.data <- Data$y
    
    
    StoreOutput_coef(t, annum)
  }
  
  getOutput_2000(annum = 2000)
  
  # 2001:2020
  for (annum in 2001:2020) {
    filepath = paste0("pop_3set_00to20.xlsx")
    year1990 <- read_excel(filepath, sheet = paste0('year', annum))
    
    year1990 =  tidyDF_0120(year1990)
    train_df = year1990
    
    
    Data = getData_0120(train_df, annum)
    train.data = Data$x
    test.data <- Data$y
    
    
    StoreOutput_coef(t, annum)
  }
  
}


```

#### 1 Repeat training for 50-500 times

```{R}
train_times = c(50,100,200,300,400,500)
for (i in train_times) {
  get_t_times_accuracy(i)
}

```

#### 2 Fetch results to table

```{R}
txt_path = paste0("Fig_repeat_training_best_times")
setwd(txt_path)

files <- list.files(txt_path,pattern = "*.txt") #,full.names = TRUE

# Create an empty data frame
txt_data <- data.frame(file_name = character(),
                        first_line = character(),
                        second_line = character(),
                        third_line = character(),
                        stringsAsFactors = FALSE)

# Loop through each TXT file and extract the first two lines
for (file in files) {

  lines <- readLines(file, n = 3)
  
  # Add the filename and the first two rows to the data frame
  txt_data <- rbind(txt_data, data.frame(file_name = file,
                                         first_line = lines[1],
                                         second_line = lines[2],
                                         third_line = lines[3],
                                         stringsAsFactors = FALSE))
}

# check output
View(txt_data)

csv_file = "Origin_repeat_training_best_times.csv"
write.csv(txt_data,csv_file)

```



